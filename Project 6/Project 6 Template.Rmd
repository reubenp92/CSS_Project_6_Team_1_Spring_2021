---
title: 'Project 6: Randomization and Matching'
output: pdf_document
---

# Introduction

In this project, you will explore the question of whether college education causally affects political participation. Specifically, you will use replication data from \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1409483}{Who Matches? Propensity Scores and Bias in the Causal Eï¬€ects of Education on Participation} by former Berkeley PhD students John Henderson and Sara Chatfield. Their paper is itself a replication study of \href{https://www.jstor.org/stable/10.1017/s0022381608080651}{Reconsidering the Effects of Education on Political Participation} by Cindy Kam and Carl Palmer. In their original 2008 study, Kam and Palmer argue that college education has no effect on later political participation, and use the propensity score matching to show that pre-college political activity drives selection into college and later political participation. Henderson and Chatfield in their 2011 paper argue that the use of the propensity score matching in this context is inappropriate because of the bias that arises from small changes in the choice of variables used to model the propensity score. They use \href{http://sekhon.berkeley.edu/papers/GenMatch.pdf}{genetic matching} (at that point a new method), which uses an approach similar to optimal matching to optimize Mahalanobis distance weights. Even with genetic matching, they find that balance remains elusive however, thus leaving open the question of whether education causes political participation.

You will use these data and debates to investigate the benefits and pitfalls associated with matching methods. Replication code for these papers is available online, but as you'll see, a lot has changed in the last decade or so of data science! Throughout the assignment, use tools we introduced in lab from the \href{https://www.tidyverse.org/}{tidyverse} and the \href{https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf}{MatchIt} packages. Specifically, try to use dplyr, tidyr, purrr, stringr, and ggplot instead of base R functions. While there are other matching software libraries available, MatchIt tends to be the most up to date and allows for consistent syntax.

# Data

The data is drawn from the \href{https://www.icpsr.umich.edu/web/ICPSR/studies/4023/datadocumentation#}{Youth-Parent Socialization Panel Study} which asked students and parents a variety of questions about their political participation. This survey was conducted in several waves. The first wave was in 1965 and established the baseline pre-treatment covariates. The treatment is whether the student attended college between 1965 and 1973 (the time when the next survey wave was administered). The outcome is an index that calculates the number of political activities the student engaged in after 1965. Specifically, the key variables in this study are:

\begin{itemize}
    \item \textbf{college}: Treatment of whether the student attended college or not. 1 if the student attended college between 1965 and 1973, 0 otherwise.
    \item \textbf{ppnscal}: Outcome variable measuring the number of political activities the student participated in. Additive combination of whether the student voted in 1972 or 1980 (student\_vote), attended a campaign rally or meeting (student\_meeting), wore a campaign button (student\_button), donated money to a campaign (student\_money), communicated with an elected official (student\_communicate), attended a demonstration or protest (student\_demonstrate), was involved with a local community event (student\_community), or some other political participation (student\_other)
\end{itemize}

Otherwise, we also have covariates measured for survey responses to various questions about political attitudes. We have covariates measured for the students in the baseline year, covariates for their parents in the baseline year, and covariates from follow-up surveys. \textbf{Be careful here}. In general, post-treatment covariates will be clear from the name (i.e. student\_1973Married indicates whether the student was married in the 1973 survey). Be mindful that the baseline covariates were all measured in 1965, the treatment occurred between 1965 and 1973, and the outcomes are from 1973 and beyond. We will distribute the Appendix from Henderson and Chatfield that describes the covariates they used, but please reach out with any questions if you have questions about what a particular variable means.

```{r}
# Load tidyverse and MatchIt
# Feel free to load other libraries as you wish
library(tidyverse)
library(MatchIt)

# Load ypsps data
ypsps <- read_csv('data/ypsps.csv')
head(ypsps)
```

# Randomization

Matching is usually used in observational studies to to approximate random assignment to treatment. But could it be useful even in randomized studies? To explore the question do the following:

\begin{enumerate}
    \item Generate a vector that randomly assigns each unit to either treatment or control
    \item Choose a baseline covariate (for either the student or parent). A binary covariate is probably best for this exercise.
    \item Visualize the distribution of the covariate by treatment/control condition. Are treatment and control balanced on this covariate?
    \item Simulate the first 3 steps 10,000 times and visualize the distribution of treatment/control balance across the simulations.
\end{enumerate}

```{r}
# Generate a vector that randomly assigns each unit to treatment/control
set.seed(13)
m = nrow(ypsps) #number of rows in dataframe = 1254

ypsps_df <- ypsps %>% 
    mutate(A = sample(rep(c(0,1), each = m/2)))
```

```{r}
# Choose a baseline covariate (use dplyr for this) 
df_1 = filter(ypsps_df, A==1)   #filtering dataframe by A
df_0 = filter(ypsps_df, A==0)

mean(df_1$parent_Magazine)  
mean(df_0$parent_Magazine)  

#We chose "parent_Magazine," which is a binary variable indicating whether parents read about public affairs and politics in any magazine.

```

```{r}
# Visualize the distribution by treatment/control (ggplot). Are treatment and control balanced on this covariate?

ggplot(ypsps_df, aes(x = parent_Magazine, fill = factor(A))) + 
  geom_bar() +
  facet_grid(A~.) + 
  labs(title = "Distribution of Parents Reading Magazines about Politics at Baseline", fill = "A\n")

#2x2 table of T vs C and whether parents read vs. not read political magazines.
table(ypsps_df$A, ypsps_df$parent_Magazine) 

#testing whether parents in treatment and control groups are significant different in whether or not they reading political magazines
chisq.test(table(ypsps_df$A, ypsps_df$parent_Magazine))  

#A: The treatment and control groups are not significantly different in parents' reading political magazines at baseline, since the p-value is greater than .05. Treatment and control are balanced on this covariate.

```

```{r}
# Simulate this 10,000 times (monte carlo simulation - see R Refresher for a hint)

# Number of simulations
simnum <- 10000

#creating empty lists 
meanlist1 <- list() #store the proportion of 1s for Treatment(1) 
meanlist0 <- list() #store the proportion of 1s for Control(0) group
pvallist <- list() #store the p-values from Chi-squared tests each iteration

#Finding the proportion of 1s (parents' reading political magazines) in T and C groups, 
#iterating through 10000 simulations

for(i in 1:simnum){
    ypsps_mc <- ypsps %>% 
      mutate(A = sample(rep(c(0,1), each = m/2)))
    eachmean1 <- mean(ypsps_mc$parent_Magazine[ypsps_mc$A==1])  #finding proportion(mean)
    eachmean0 <- mean(ypsps_mc$parent_Magazine[ypsps_mc$A==0])
    meanlist1 <- c(meanlist1, eachmean1)   #storing means
    meanlist0 <- c(meanlist0, eachmean0)
    chitest = chisq.test(table(ypsps_mc$A, ypsps_mc$parent_Magazine))  #testing difference
    pval = chitest$p.value
    pvallist <- c(pvallist, pval)    #storing p-values
    ypsps_mc <- select(ypsps_mc, -A)    #deleting A column to reset the dataframe
}

#taking a look at the chi-squared test p-values from each iteration
sort(unlist(pvallist)) 
hist(unlist(pvallist))

#Density plots of the proportion of parents that read political magazines in Treatment vs. Control groups
treatmean <- data.frame(values = matrix(unlist(meanlist1), byrow=TRUE), stringsAsFactors=FALSE)
controlmean <- data.frame(values = matrix(unlist(meanlist0), byrow=TRUE), stringsAsFactors=FALSE)

treatmean$mean <- 'Treat'
controlmean$mean <- 'Control'

meandata <- rbind(treatmean, controlmean)
meandata
ggplot(meandata, aes(values, fill = mean)) + geom_density(alpha = 0.2)


```

## Questions
\begin{enumerate}
    \item \textbf{What do you see across your simulations? Why does independence of treatment assignment and baseline covariates not guarantee balance of treatment assignment and baseline covariates?}
\end{enumerate}

\textbf{Your Answer}: Simulating the randomization and examination of baseline covariates 10000 times produces an approximately normal, and similar density plots for both treatment and control groups. In other words, we randomly assigned treatment (through vector A) and saw whether the proportion of parents that read political magazines is significantly different at baseline between treated and control groups, and found that on average across simulations they are not different at a statistically significant level. 

However, as the normally distributed graphs show, the left and right tails of the graph suggest that the proportions may be very different for the outliers. At the extremes, it may mean that the treated group had a huge majority of parents read political magazines (to the point of parent_Magazine==1), and vice versa. 

Taking a look at the p-values from running the chi-squared test comparing the treatment and control groups (regarding whether parents read political magazines) each iteration further confirms this idea. A p-value of less than .05 suggests that the treatment and control groups are statistically different and hence the covariate is unbalanced at baseline. The list of p-values ('pvallist') shows around 400 iterations producing p-values less than .05. This further confirms the idea that the independence of treatment assignment and baseline covariates does not guarantee balance of treatment assignment and baseline covariates. 


# Propensity Score Matching

## One Model
Select covariates that you think best represent the "true" model predicting whether a student chooses to attend college, and estimate a propensity score model to calculate the Average Treatment Effect on the Treated (ATT). Plot the balance of the top 10 (or fewer if you select fewer covariates). Report the balance of the p-scores across both the treatment and control groups, and using a threshold of standardized mean difference of p-score $\leq .1$, report the number of covariates that meet that balance threshold.

```{r}
# Select covariates that represent the "true" model for selection, fit model
match_full_ate <- matchit(formula = college ~ parent_Money + parent_EducHH + parent_Race + student_GPA + student_community + student_FamTalk, method = 'full', data = ypsps, estimand = 'ATE')
                         
summary(match_full_ate)
```

```{r}
# Plot the balance for the top 10 covariates
plot(match_full_ate, type = "qq", interactive = FALSE,
     which.xs = c("parent_Money", "parent_EducHH", "parent_Race", "student_GPA", "student_community", "student_FamTalk"))

ate.sum <- summary(match_full_ate)
plot(ate.sum, var.order="unmatched", threshold = c(.1,.05))
```


```{r}
# Report the overall balance and the proportion of covariates that meet the balance threshold
sum_matched<-ate.sum$sum.matched
sum_matched<-as.data.frame(sum_matched)

Treat_score <-sum_matched[1] #means treated
Control_score <-sum_matched[2] #means control
smd_balance <- sum_matched [3] %>% filter_all(all_vars(.<=.1)) # variables with standard mean difference <=.1

```


Henderson/Chatfield argue that an improperly specified propensity score model can actually \textit{increase} the bias of the estimate. To demonstrate this, they simulate 800,000 different propensity score models by choosing different permutations of covariates. To investigate their claim, do the following:

\begin{itemize}
    \item Using as many simulations as is feasible (at least 10,000 should be ok, more is better!), randomly select the number of and the choice of covariates for the propensity score model.
    \item For each run, store the ATT, the proportion of covariates that meet the standardized mean difference $\leq .1$ threshold, and the mean percent improvement in the standardized mean difference. You may also wish to store the entire models in a list and extract the relevant attributes as necessary.
    \item Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.
    \item Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like \href{https://cran.r-project.org/web/packages/gridExtra/index.html}{gridExtra} to arrange these)
\end{itemize}

\textbf{Note: There are lots of post-treatment covariates in this dataset (about 50!)! You need to be careful not to include these in the pre-treatment balancing. Many of you are probably used to selecting or dropping columns manually, or positionally. However, you may not always have a convenient arrangement of columns, nor is it fun to type out 50 different column names. Instead see if you can use dplyr 1.0.0 functions to programatically drop post-treatment variables (\href{https://www.tidyverse.org/blog/2020/03/dplyr-1-0-0-select-rename-relocate/}{here} is a useful tutorial).}

```{r}
# Remove post-treatment covariates
set.seed(10)

ypsps_pc <- ypsps %>% 
  select(!(contains('1973')|contains('1982')))

# Simulate random selection of features 10k+ times
s <-10

# create an empty list for storing the models
models_list <- list()

# create empty vectors for storage
ATT_full <- c()
smd_prop <- c()
smd_mean <- c()

# loop through random numbers of random covariates and create matchit model objects
for(i in 1:s){
  samp_df <- ypsps_pc %>% select(-c(college,student_ppnscal)) # drop treatment and outcome for the covariate sampling
  # grab a random number of random column indices
  covs <- sample(1:ncol(samp_df),sample(1:ncol(samp_df),1))
  # subset the dataframe using the vector of column indices
  m_df <- ypsps_pc %>% select(c(covs,"college","student_ppnscal")) # always keep the treatment indicator and the outcome
  mdf_nomiss <- m_df %>% drop_na() # MatchIt does not allow missing values
  
  # construct character string for formula, subtracting treatment and outcome
  cov_names <- names(select(mdf_nomiss,-c(college,student_ppnscal)))
  form_char <- paste0("college ~ ",paste0(cov_names,collapse=' + '), sep = " ")
  
  # run the matching algothrim on all the variables in the df
  result <- matchit(formula = as.formula(form_char), data = mdf_nomiss, method = "full", distance = "glm") # store it
  models_list[[i]] <- result
  match_full_att_summ <- summary(result) # get summary object
  # store prop std mean diff meeting the >= .1 threshold
  smd_prop[i] <- sum(match_full_att_summ$sum.all[,3] <= .1)/length(match_full_att_summ$sum.all[,3])
  # store mean pct improvement in std mean diff
  smd_mean[i] <- mean(match_full_att_summ$reduction[,1])
  match_full_att_data <- match.data(result) # recover matched dataset
  form_char2 <- paste0("student_ppnscal ~ ",paste0(cov_names,collapse=' + '), sep = " + college") # construct character string for second formula
  # estimate the ATT
  lm_full_att <- lm(as.formula(form_char2), data = match_full_att_data, weights = weights)
  ATT_full[i] <- lm_full_att$coefficients["college"] # store estimate for college
}
```

```{r}
plots <- data.frame(ATT_full, smd_mean, smd_prop)
```

```{r}
# Plot ATT v. proportion
p <- ggplot(plots, aes(x = smd_prop, y = ATT_full)) +
  geom_point(color = "blue") +
  xlab('% covariates meeting .1 threshold') +
  ylab('ATT') +
  ggtitle('ATT versus Proportion') 
p
# 10 random covariate balance plots (hint try gridExtra)

# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!
```
```{r}
library(cobalt)
library(gridExtra)
model_inds <- sample(length(models_list),10) # sample 10 models and get their indices
p <- lapply(model_inds, function(i) love.plot(models_list[[i]], binary = "std"))
do.call("grid.arrange", p)

```

```{r}
install.packages('cobalt')
```

```{r}
library(cobalt)
library(gridExtra)
model_inds <- sample(length(models_list),10) # sample 10 models and get their indices
p <- lapply(model_inds, function(i) love.plot(models_list[[i]], binary = "std"))
do.call("grid.arrange", p)
```

## Questions

\begin{enumerate}
    \item \textbf{How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?}
    \item \textbf{Your Answer}:
    \item \textbf{Analyze the distribution of the ATTs. Do you have any concerns about this distribution?}
    \item \textbf{Your Answer:}
    \item \textbf{Do your 10 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?}
    \item \textbf{Your Answer:}
\end{enumerate}

# Matching Algorithm of Your Choice

## Simulate Alternative Model

Henderson/Chatfield propose using genetic matching to learn the best weights for Mahalanobis distance matching. Choose a matching algorithm other than the propensity score (you may use genetic matching if you wish, but it is also fine to use the greedy or optimal algorithms we covered in lab instead). Repeat the same steps as specified in Section 4.2 and answer the following questions:


```{r}
set.seed(10)

# Remove post-treatment covariates
ypsps_precovs <- ypsps_df %>% 
  select(!(contains('1973')|contains('1982')))

sims_n <- 10 # set the number of simulations

# create an empty list for storing the models
models_list <- list()

# create empty vectors for storage
ATT_full <- c()
smd_prop <- c()
smd_mean <- c()

# loop through random numbers of random covariates and create matchit model objects

for(i in 1:sims_n){
  samp_df <- ypsps_precovs %>% select(-c(college,student_ppnscal)) # drop treatment and outcome for the covariate sampling
  # grab a random number of random column indices
  covs <- sample(1:ncol(samp_df),sample(1:ncol(samp_df),1))
  # subset the dataframe using the vector of column indices
  m_df <- ypsps_precovs %>% select(c(covs,"college","student_ppnscal")) # always keep the treatment indicator and the outcome
  mdf_nomiss <- m_df %>% drop_na() # MatchIt does not allow missing values
  # construct character string for formula, subtracting treatment and outcome
  cov_names <- names(select(mdf_nomiss,-c(college,student_ppnscal)))
  form_char <- paste0("college ~ ",paste0(cov_names,collapse=' + '), sep = " ")
  # run the matching algothrim on all the variables in the df
  result <- matchit(formula = as.formula(form_char), data = mdf_nomiss, method = "full", distance = "mahalanobis") # store it
  models_list[[i]] <- result
  match_full_att_summ <- summary(result) # get summary object
  # store prop std mean diff meeting the >= .1 threshold
  smd_prop[i] <- sum(match_full_att_summ$sum.all[,3] <= .1)/length(match_full_att_summ$sum.all[,3])
  # store mean pct improvement in std mean diff
  smd_mean[i] <- mean(match_full_att_summ$reduction[,1])
  match_full_att_data <- match.data(result) # recover matched dataset
  form_char2 <- paste0("student_ppnscal ~ ",paste0(cov_names,collapse=' + '), sep = " + college") # construct character string for second formula
  # estimate the ATT
  lm_full_att <- lm(as.formula(form_char2), data = match_full_att_data, weights = weights)
  ATT_full[i] <- lm_full_att$coefficients["college"] # store estimate for college
}

# create df of stored values

plots_df <- data.frame(ATT_full,smd_mean,smd_prop)

# Plot ATT v. proportion

p <- ggplot(plots_df, aes(x = smd_prop, y = ATT_full)) +
  geom_point() +
  xlab('% covariates meeting .1 threshold') +
  ylab('ATT') +
  ggtitle('ATT versus proportion')
p

# 10 random covariate balance plots (hint try gridExtra)
library(cobalt)
library(gridExtra)

model_inds <- sample(length(models_list),10) # sample 10 models and get their indices

p <- lapply(model_inds, function(i) love.plot(models_list[[i]], binary = "std"))
do.call("grid.arrange", p)
```

```{r}
# Visualization for distributions of percent improvement
```

## Questions

\begin{enumerate}
    \item \textbf{Does your alternative matching method have more runs with higher proportions of balanced covariates?}
    \item \textbf{Your Answer:}
    \item \textbf{Use a visualization to examine the change in the distribution of the percent improvement in balance in propensity score matching vs. the distribution of the percent improvement in balance in your new method. Which did better? Analyze the results in 1-2 sentences.}
    \item \textbf{Your Answer:}
\end{enumerate}

\textbf{Optional:} Looking ahead to the discussion questions, you may choose to model the propensity score using an algorithm other than logistic regression and perform these simulations again, if you wish to explore the second discussion question further.

# Discussion Questions

\begin{enumerate}
    \item Why might it be a good idea to do matching even if we have a randomized or as-if-random design?
    \item \textbf{Your Answer:} The purpose of matching is to find two groups that are balanced on baseline covariates so that they are comparable across all characteristics except the treatment itself. This allows us to measure the average effect of treatment between the treated and control groups. In theory, randomization or as-if-random design is supposed to achieve the same goal, by randomly assigning the treatment so that the probability of receiving the treatment is equal across the treatment and control groups, and thus make the two groups comparable. However, as we have shown, randomization, that is, the independence of treatment assignment and baseline covariates, does not guarantee balance of treatment assignment and baseline covariates. We find that on many occasions, random assignment of treatment produces imbalance between the treatment and control groups. Therefore, using matching methods, even if we have randomization, helps to make sure that the two groups are balanced and comparable. 

    \item The standard way of estimating the propensity score is using a logistic regression to estimate probability of treatment. Given what we know about the curse of dimensionality, do you think there might be advantages to using other machine learning algorithms (decision trees, bagging/boosting forests, ensembles, etc.) to estimate propensity scores instead?
    \item \textbf{Your Answer:} Although logistic regression is the standard way of estimating the probability of treatment in propensity score method, it suffers from disadvantages. For example, since the propensity score using logistic regression requires the inclusion of all covariates potentially related to treatment assignment, it suffers from the curse of dimensionality in that it is difficult to incorporate and interpret each variableâ€™s contribution in the model. Similarly, with so many covariates (high dimensionality), it is difficult to interpret the interactions and relationships among the covariates, and the researcher needs to specify from the beginning what interaction terms she expects. Therefore, there might be advantages to using other machine learning algorithms in place of logistic regression. For example, machine learning methods (e.g. neural networks and SVMs) would allow for high dimensionality and be able to incorporate interaction terms without having to determine from the outset what relationships might exist among variables as in logistic regression. They also may deal better with overfitting and nonlinearities. 

\end{enumerate}